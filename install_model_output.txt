
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing ALL models
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: ltx-video
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: ltx-video
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: ltx-video
   âœ… Virtual environment created: ltx-video

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Weights download on first run
   âœ… ltx-video setup complete
   âœ… ltx-video installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: ltx-video-13b-distilled
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: ltx-video-13b-distilled
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: ltx-video-13b-distilled
   âœ… Virtual environment created: ltx-video-13b-distilled

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Weights download on first run
   âœ… ltx-video-13b-distilled setup complete
   âœ… ltx-video-13b-distilled installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: svd
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: svd
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: svd
   âœ… Virtual environment created: svd

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Weights download on first run
   âœ… svd setup complete
   âœ… svd installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: morphic-frames-to-video
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: morphic-frames-to-video
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: morphic-frames-to-video
   âœ… Virtual environment created: morphic-frames-to-video

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â­ï¸  Wan2.2-I2V-A14B weights exist
   â­ï¸  Morphic LoRA weights exist

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Creating Symlinks
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â­ï¸  Weights symlink already exists
   âœ… morphic-frames-to-video setup complete
   âœ… morphic-frames-to-video installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: hunyuan-video-i2v
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: hunyuan-video-i2v
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: hunyuan-video-i2v
   âœ… Virtual environment created: hunyuan-video-i2v

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Weights download on first run
   âœ… hunyuan-video-i2v setup complete
   âœ… hunyuan-video-i2v installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: dynamicrafter-256
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
System Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Checking FFmpeg system dependencies...
   âš ï¸  FFmpeg development libraries not found
ğŸ”§ Installing FFmpeg dependencies (requires sudo)...

WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

Get:1 https://nvidia.github.io/libnvidia-container/stable/deb/amd64  InRelease [1477 B]
Get:2 https://download.docker.com/linux/ubuntu jammy InRelease [48.5 kB]
Get:3 https://packages.microsoft.com/repos/azure-cli jammy InRelease [3596 B]
Ign:4 http://linux.mellanox.com/public/repo/doca/2.9.3/ubuntu22.04/x86_64 ./ InRelease
Get:5 https://download.docker.com/linux/ubuntu jammy/stable amd64 Packages [66.0 kB]
Get:6 https://nvidia.github.io/libnvidia-container/stable/deb/amd64  Packages [23.3 kB]
Get:7 https://packages.microsoft.com/repos/azure-cli jammy/main amd64 Packages [2958 B]
Hit:8 http://archive.lambdalabs.com/ubuntu jammy InRelease
Hit:9 http://linux.mellanox.com/public/repo/doca/2.9.3/ubuntu22.04/x86_64 ./ Release
Get:11 https://pkg.cloudflare.com/cloudflared jammy InRelease [5043 B]
Get:12 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]
Get:13 http://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]
Hit:14 http://archive.ubuntu.com/ubuntu jammy InRelease
Get:15 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]
Get:16 https://pkg.cloudflare.com/cloudflared jammy/main amd64 Packages [377 B]
Get:17 http://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.4 kB]
Hit:18 https://ppa.launchpadcontent.net/flexiondotorg/nvtop/ubuntu jammy InRelease
Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2899 kB]
Get:20 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]
Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3161 kB]
Get:22 http://security.ubuntu.com/ubuntu jammy-security/main Translation-en [417 kB]
Get:23 http://security.ubuntu.com/ubuntu jammy-security/main amd64 c-n-f Metadata [14.0 kB]
Get:24 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4883 kB]
Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main Translation-en [485 kB]
Get:26 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 c-n-f Metadata [19.0 kB]
Get:27 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5044 kB]
Get:28 http://security.ubuntu.com/ubuntu jammy-security/restricted Translation-en [917 kB]
Get:29 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 c-n-f Metadata [652 B]
Get:30 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1007 kB]
Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/restricted Translation-en [944 kB]
Get:32 http://security.ubuntu.com/ubuntu jammy-security/universe Translation-en [221 kB]
Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 c-n-f Metadata [644 B]
Get:34 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1244 kB]
Get:35 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 c-n-f Metadata [22.3 kB]
Get:36 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [50.5 kB]
Get:37 http://security.ubuntu.com/ubuntu jammy-security/multiverse Translation-en [10.2 kB]
Get:38 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 c-n-f Metadata [376 B]
Get:39 http://archive.ubuntu.com/ubuntu jammy-updates/universe Translation-en [310 kB]
Get:40 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 c-n-f Metadata [30.0 kB]
Get:41 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [57.7 kB]
Get:42 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 c-n-f Metadata [600 B]
Get:43 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [93.6 kB]
Get:44 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 c-n-f Metadata [412 B]
Get:45 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [34.1 kB]
Get:46 http://archive.ubuntu.com/ubuntu jammy-backports/universe Translation-en [16.7 kB]
Get:47 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 c-n-f Metadata [672 B]
Fetched 22.5 MB in 4s (5291 kB/s)
Reading package lists...
Building dependency tree...
Reading state information...
54 packages can be upgraded. Run 'apt list --upgradable' to see them.

WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

Reading package lists...
Building dependency tree...
Reading state information...
ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).
The following package was automatically installed and is no longer required:
  libpkgconf3
Use 'sudo apt autoremove' to remove it.
The following packages will be REMOVED:
  pkgconf
The following NEW packages will be installed:
  libavcodec-dev libavdevice-dev libavfilter-dev libavformat-dev libavutil-dev
  libpostproc-dev libswresample-dev libswscale-dev pkg-config
0 upgraded, 9 newly installed, 1 to remove and 54 not upgraded.
Need to get 10.2 MB of archives.
After this operation, 39.1 MB of additional disk space will be used.
Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 pkg-config amd64 0.29.2-1ubuntu3 [48.2 kB]
Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavutil-dev amd64 7:4.4.2-0ubuntu0.22.04.1 [427 kB]
Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libswresample-dev amd64 7:4.4.2-0ubuntu0.22.04.1 [78.0 kB]
Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavcodec-dev amd64 7:4.4.2-0ubuntu0.22.04.1 [6221 kB]
Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavformat-dev amd64 7:4.4.2-0ubuntu0.22.04.1 [1347 kB]
Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libpostproc-dev amd64 7:4.4.2-0ubuntu0.22.04.1 [60.9 kB]
Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libswscale-dev amd64 7:4.4.2-0ubuntu0.22.04.1 [206 kB]
Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavfilter-dev amd64 7:4.4.2-0ubuntu0.22.04.1 [1732 kB]
Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavdevice-dev amd64 7:4.4.2-0ubuntu0.22.04.1 [97.3 kB]
Fetched 10.2 MB in 1s (7492 kB/s)
dpkg: pkgconf: dependency problems, but removing anyway as you requested:
 knem-dkms depends on pkg-config; however:
  Package pkg-config is not installed.
  Package pkgconf which provides pkg-config is to be removed.
 knem depends on pkg-config; however:
  Package pkg-config is not installed.
  Package pkgconf which provides pkg-config is to be removed.

(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 213607 files and directories currently installed.)
Removing pkgconf (1.8.0-1) ...
Removing 'diversion of /usr/bin/pkg-config to /usr/bin/pkg-config.real by pkgconf'
Removing 'diversion of /usr/share/aclocal/pkg.m4 to /usr/share/aclocal/pkg.real.m4 by pkgconf'
Removing 'diversion of /usr/share/man/man1/pkg-config.1.gz to /usr/share/man/man1/pkg-config.real.1.gz by pkgconf'
Removing 'diversion of /usr/share/pkg-config-crosswrapper to /usr/share/pkg-config-crosswrapper.real by pkgconf'
Selecting previously unselected package pkg-config.
(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 213587 files and directories currently installed.)
Preparing to unpack .../0-pkg-config_0.29.2-1ubuntu3_amd64.deb ...
Unpacking pkg-config (0.29.2-1ubuntu3) ...
Selecting previously unselected package libavutil-dev:amd64.
Preparing to unpack .../1-libavutil-dev_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...
Unpacking libavutil-dev:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...
Selecting previously unselected package libswresample-dev:amd64.
Preparing to unpack .../2-libswresample-dev_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...
Unpacking libswresample-dev:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...
Selecting previously unselected package libavcodec-dev:amd64.
Preparing to unpack .../3-libavcodec-dev_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...
Unpacking libavcodec-dev:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...
Selecting previously unselected package libavformat-dev:amd64.
Preparing to unpack .../4-libavformat-dev_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...
Unpacking libavformat-dev:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...
Selecting previously unselected package libpostproc-dev:amd64.
Preparing to unpack .../5-libpostproc-dev_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...
Unpacking libpostproc-dev:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...
Selecting previously unselected package libswscale-dev:amd64.
Preparing to unpack .../6-libswscale-dev_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...
Unpacking libswscale-dev:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...
Selecting previously unselected package libavfilter-dev:amd64.
Preparing to unpack .../7-libavfilter-dev_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...
Unpacking libavfilter-dev:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...
Selecting previously unselected package libavdevice-dev:amd64.
Preparing to unpack .../8-libavdevice-dev_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...
Unpacking libavdevice-dev:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...
Setting up libavutil-dev:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...
Setting up libswresample-dev:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...
Setting up libavcodec-dev:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...
Setting up libpostproc-dev:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...
Setting up libavformat-dev:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...
Setting up libswscale-dev:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...
Setting up pkg-config (0.29.2-1ubuntu3) ...
Setting up libavfilter-dev:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...
Setting up libavdevice-dev:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...
Processing triggers for man-db (2.10.2-1) ...
   âœ… FFmpeg dependencies installed

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: dynamicrafter-256
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: dynamicrafter-256
   âœ… Virtual environment created: dynamicrafter-256

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â­ï¸  Checkpoint exists: model.ckpt
   âœ… dynamicrafter-256 setup complete
   âœ… dynamicrafter-256 installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: dynamicrafter-512
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
System Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Checking FFmpeg system dependencies...
   âœ… FFmpeg libraries already installed

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: dynamicrafter-512
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: dynamicrafter-512
   âœ… Virtual environment created: dynamicrafter-512

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â­ï¸  Checkpoint exists: model.ckpt
   âœ… dynamicrafter-512 setup complete
   âœ… dynamicrafter-512 installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: dynamicrafter-1024
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
System Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Checking FFmpeg system dependencies...
   âœ… FFmpeg libraries already installed

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: dynamicrafter-1024
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: dynamicrafter-1024
   âœ… Virtual environment created: dynamicrafter-1024

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â­ï¸  Checkpoint exists: model.ckpt
   âœ… dynamicrafter-1024 setup complete
   âœ… dynamicrafter-1024 installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: videocrafter2-512
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
System Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Checking FFmpeg system dependencies...
   âœ… FFmpeg libraries already installed

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: videocrafter2-512
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: videocrafter2-512
   âœ… Virtual environment created: videocrafter2-512

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â­ï¸  Checkpoint exists: model.ckpt
   âœ… videocrafter2-512 setup complete
   âœ… videocrafter2-512 installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: cogvideox-5b-i2v
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
System Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Checking FFmpeg system dependencies...
   âœ… FFmpeg libraries already installed

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: cogvideox-5b-i2v
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: cogvideox-5b-i2v
   âœ… Virtual environment created: cogvideox-5b-i2v

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model Weights
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Model weights will be downloaded on first run (~11GB)
   ğŸ“Œ HuggingFace repo: THUDM/CogVideoX-5b-I2V
   ğŸ“Œ Cache location: ~/.cache/huggingface/hub/models--THUDM--CogVideoX-5b-I2V
   âœ… cogvideox-5b-i2v setup complete
   ğŸ“Œ Generated videos: 6 seconds (49 frames @ 8fps) at 720x480 resolution
   ğŸ“Œ GPU Memory: ~10GB with optimizations (sequential offload + VAE tiling)
   âœ… cogvideox-5b-i2v installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: cogvideox1.5-5b-i2v
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
System Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Checking FFmpeg system dependencies...
   âœ… FFmpeg libraries already installed

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: cogvideox1.5-5b-i2v
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: cogvideox1.5-5b-i2v
   âœ… Virtual environment created: cogvideox1.5-5b-i2v

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model Weights
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Model weights will be downloaded on first run (~11GB)
   ğŸ“Œ HuggingFace repo: THUDM/CogVideoX1.5-5B-I2V
   ğŸ“Œ Cache location: ~/.cache/huggingface/hub/models--THUDM--CogVideoX1.5-5B-I2V
   âœ… cogvideox1.5-5b-i2v setup complete
   ğŸ“Œ Generated videos: 10 seconds (81 frames @ 16fps) at 1360x768 resolution
   ğŸ“Œ GPU Memory: ~10GB with optimizations (sequential offload + VAE tiling)
   âœ… cogvideox1.5-5b-i2v installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: sana-video-2b-480p
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: sana-video-2b-480p
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: sana-video-2b-480p
   âœ… Virtual environment created: sana-video-2b-480p

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Weights download on first run via HuggingFace
   âœ… sana-video-2b-480p setup complete
   âœ… sana-video-2b-480p installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: sana-video-2b-longlive
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: sana-video-2b-longlive
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: sana-video-2b-longlive
   âœ… Virtual environment created: sana-video-2b-longlive

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Weights download on first run via HuggingFace
   âœ… sana-video-2b-longlive setup complete
   âœ… sana-video-2b-longlive installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: wan-2.2-i2v-a14b
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: wan
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: wan
   âœ… Virtual environment created: wan

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Weights download on first run
   âœ… wan setup complete
   âœ… wan-2.2-i2v-a14b installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: luma-ray-2
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: luma-ray-2
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: luma-ray-2
   âœ… Virtual environment created: luma-ray-2

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API Configuration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… LUMA_API_KEY configured (luma-aa2...25b3)
   âœ… luma-ray-2 setup complete
   âœ… luma-ray-2 installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: luma-ray-flash-2
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: luma-ray-flash-2
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: luma-ray-flash-2
   âœ… Virtual environment created: luma-ray-flash-2

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API Configuration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… LUMA_API_KEY configured (luma-aa2...25b3)
   âœ… luma-ray-flash-2 setup complete
   âœ… luma-ray-flash-2 installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: veo-2
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: veo-2
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: veo-2
   âœ… Virtual environment created: veo-2

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API Configuration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… GEMINI_API_KEY configured
   âœ… veo-2 setup complete
   âœ… veo-2 installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: veo-3.0-generate
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: veo-3.0-generate
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: veo-3.0-generate
   âœ… Virtual environment created: veo-3.0-generate

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API Configuration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… GEMINI_API_KEY configured
   âœ… veo-3.0-generate setup complete
   âœ… veo-3.0-generate installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: veo-3.1-fast
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: veo-3.1-fast
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: veo-3.1-fast
   âœ… Virtual environment created: veo-3.1-fast

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API Configuration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… WAVESPEED_API_KEY configured
   âœ… veo-3.1-fast setup complete
   âœ… veo-3.1-fast installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: wavespeed-wan-2.1-i2v-480p
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: wavespeed-wan2.1
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: wavespeed-wan2.1
   âœ… Virtual environment created: wavespeed-wan2.1

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Checkpoints
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   ğŸ“Œ Weights download on first run
   âœ… wavespeed-wan2.1 setup complete
   âœ… wavespeed-wan-2.1-i2v-480p installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: runway-gen4-turbo
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: runway-gen4-turbo
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: runway-gen4-turbo
   âœ… Virtual environment created: runway-gen4-turbo

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API Configuration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… RUNWAYML_API_SECRET configured (key_ff2e...37ad)
   âœ… runway-gen4-turbo setup complete
   âœ… runway-gen4-turbo installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installing: openai-sora-2
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Virtual Environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Removing existing environment: openai-sora-2
   âœ… Old environment removed
ğŸ”§ Creating virtual environment: openai-sora-2
   âœ… Virtual environment created: openai-sora-2

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dependencies
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API Configuration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… OPENAI_API_KEY configured (sk-proj-..._1cA)
   âœ… openai-sora-2 setup complete
   âœ… openai-sora-2 installed successfully

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Validation Phase
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: ltx-video
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating ltx-video... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): ltx-video

ğŸ” Verifying 1 model(s) for testing...
   âœ… ltx-video: LTX-Video
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - ltx-video
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: LTX-Video (ltx-video)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001
      â­ï¸  Skipped (existing output: ltx-video_tests_0001_20251217_002546)
    [2/2] Processing: tests_0002
      â­ï¸  Skipped (existing output: ltx-video_tests_0002_20251217_002627)

  ğŸ“Š Model LTX-Video Summary: 0 completed, 0 failed, 2 skipped in 0h 0m 0s

â±ï¸  Sequential execution completed in 0h 0m 0s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 0 (0.0%)
   Skipped: 2 (100.0%)
   â±ï¸ Duration: 0h 0m 0s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 0 failed | â­ï¸  2 skipped

ğŸ¤– Results by Model:
   ltx-video: âœ… 0 | âŒ 0 | â­ï¸  2

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âœ… ltx-video: 2 videos generated âœ“

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: ltx-video-13b-distilled
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating ltx-video-13b-distilled... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): ltx-video-13b-distilled

ğŸ” Verifying 1 model(s) for testing...
   âœ… ltx-video-13b-distilled: LTX-Video
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - ltx-video-13b-distilled
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: LTX-Video (ltx-video-13b-distilled)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001
      â­ï¸  Skipped (existing output: ltx-video-13b-distilled_tests_0001_20251217_002640)
    [2/2] Processing: tests_0002
      â­ï¸  Skipped (existing output: ltx-video-13b-distilled_tests_0002_20251217_002756)

  ğŸ“Š Model LTX-Video Summary: 0 completed, 0 failed, 2 skipped in 0h 0m 0s

â±ï¸  Sequential execution completed in 0h 0m 0s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 0 (0.0%)
   Skipped: 2 (100.0%)
   â±ï¸ Duration: 0h 0m 0s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 0 failed | â­ï¸  2 skipped

ğŸ¤– Results by Model:
   ltx-video-13b-distilled: âœ… 0 | âŒ 0 | â­ï¸  2

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âœ… ltx-video-13b-distilled: 2 videos generated âœ“

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: svd
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating svd... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): svd

ğŸ” Verifying 1 model(s) for testing...
   âœ… svd: Stable Video Diffusion
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - svd
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: Stable Video Diffusion (svd)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001
      â­ï¸  Skipped (existing output: svd_tests_0001_20251217_002837)
    [2/2] Processing: tests_0002
      â­ï¸  Skipped (existing output: svd_tests_0002_20251217_002929)

  ğŸ“Š Model Stable Video Diffusion Summary: 0 completed, 0 failed, 2 skipped in 0h 0m 0s

â±ï¸  Sequential execution completed in 0h 0m 0s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 0 (0.0%)
   Skipped: 2 (100.0%)
   â±ï¸ Duration: 0h 0m 0s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 0 failed | â­ï¸  2 skipped

ğŸ¤– Results by Model:
   svd: âœ… 0 | âŒ 0 | â­ï¸  2

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âœ… svd: 2 videos generated âœ“

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: morphic-frames-to-video
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating morphic-frames-to-video... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): morphic-frames-to-video

ğŸ” Verifying 1 model(s) for testing...
   âœ… morphic-frames-to-video: Morphic
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - morphic-frames-to-video
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: Morphic (morphic-frames-to-video)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with morphic-frames-to-video
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: morphic-frames-to-video (will be reused for all tasks)

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/morphic-frames-to-video/tests_task/tests_0001/morphic-frames-to-video_tests_0001_20251218_001615
   - Video: data/outputs/pilot_experiment/morphic-frames-to-video/tests_task/tests_0001/morphic-frames-to-video_tests_0001_20251218_001615/video/
   - Question data: data/outputs/pilot_experiment/morphic-frames-to-video/tests_task/tests_0001/morphic-frames-to-video_tests_0001_20251218_001615/question/
   - Metadata: data/outputs/pilot_experiment/morphic-frames-to-video/tests_task/tests_0001/morphic-frames-to-video_tests_0001_20251218_001615/metadata.json
     âŒ Failed: W1218 00:16:24.737000 13916 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] 
W1218 00:16:24.737000 13916 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] *****************************************
W1218 00:16:24.737000 13916 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1218 00:16:24.737000 13916 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] *****************************************
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
        generate(args)generate(args)

  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
        generate(args)generate(args)

  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    generate(args)    
generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

        torch._C._cuda_setDevice(device)torch._C._cuda_setDevice(device)

RuntimeErrorRuntimeError: : CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

        torch._C._cuda_setDevice(device)torch._C._cuda_setDevice(device)

RuntimeErrorRuntimeError: : CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


W1218 00:16:45.093000 13916 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 13969 closing signal SIGTERM
W1218 00:16:45.095000 13916 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 13970 closing signal SIGTERM
E1218 00:16:45.259000 13916 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 13971) of binary: /home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/bin/python3
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-12-18_00:16:45
  host      : 192-222-53-214
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 13972)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-12-18_00:16:45
  host      : 192-222-53-214
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 13973)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-12-18_00:16:45
  host      : 192-222-53-214
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 13974)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2025-12-18_00:16:45
  host      : 192-222-53-214
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 13975)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2025-12-18_00:16:45
  host      : 192-222-53-214
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 13976)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-18_00:16:45
  host      : 192-222-53-214
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 13971)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

      âŒ Failed: W1218 00:16:24.737000 13916 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] 
W1218 00:16:24.737000 13916 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] *****************************************
W1218 00:16:24.737000 13916 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1218 00:16:24.737000 13916 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] *****************************************
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
        generate(args)generate(args)

  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
        generate(args)generate(args)

  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    generate(args)    
generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

        torch._C._cuda_setDevice(device)torch._C._cuda_setDevice(device)

RuntimeErrorRuntimeError: : CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

        torch._C._cuda_setDevice(device)torch._C._cuda_setDevice(device)

RuntimeErrorRuntimeError: : CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


W1218 00:16:45.093000 13916 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 13969 closing signal SIGTERM
W1218 00:16:45.095000 13916 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 13970 closing signal SIGTERM
E1218 00:16:45.259000 13916 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 13971) of binary: /home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/bin/python3
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-12-18_00:16:45
  host      : 192-222-53-214
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 13972)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-12-18_00:16:45
  host      : 192-222-53-214
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 13973)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-12-18_00:16:45
  host      : 192-222-53-214
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 13974)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2025-12-18_00:16:45
  host      : 192-222-53-214
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 13975)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2025-12-18_00:16:45
  host      : 192-222-53-214
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 13976)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-18_00:16:45
  host      : 192-222-53-214
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 13971)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

    [2/2] Processing: tests_0002

  ğŸ¬ Generating: tests_0002 with morphic-frames-to-video
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0002/first_frame.png
     Prompt: The image shows a horizontally flipped clock. After 2 hours passes on the origin...

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/morphic-frames-to-video/tests_task/tests_0002/morphic-frames-to-video_tests_0002_20251218_001645
   - Video: data/outputs/pilot_experiment/morphic-frames-to-video/tests_task/tests_0002/morphic-frames-to-video_tests_0002_20251218_001645/video/
   - Question data: data/outputs/pilot_experiment/morphic-frames-to-video/tests_task/tests_0002/morphic-frames-to-video_tests_0002_20251218_001645/question/
   - Metadata: data/outputs/pilot_experiment/morphic-frames-to-video/tests_task/tests_0002/morphic-frames-to-video_tests_0002_20251218_001645/metadata.json
     âŒ Failed: W1218 00:16:48.309000 14002 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] 
W1218 00:16:48.309000 14002 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] *****************************************
W1218 00:16:48.309000 14002 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1218 00:16:48.309000 14002 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] *****************************************
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
    generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
        generate(args)generate(args)

  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
        torch.cuda.set_device(local_rank)torch.cuda.set_device(local_rank)

  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
        torch._C._cuda_setDevice(device)torch._C._cuda_setDevice(device)

RuntimeErrorRuntimeError: : CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


        torch.cuda.set_device(local_rank)torch.cuda.set_device(local_rank)

  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
            torch._C._cuda_setDevice(device)torch._C._cuda_setDevice(device)torch._C._cuda_setDevice(device)


RuntimeErrorRuntimeError: : RuntimeErrorCUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
: 

CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

W1218 00:16:59.997000 14002 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 14080 closing signal SIGTERM
W1218 00:16:59.997000 14002 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 14081 closing signal SIGTERM
E1218 00:17:00.161000 14002 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 14082) of binary: /home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/bin/python3
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-12-18_00:16:59
  host      : 192-222-53-214
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 14083)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-12-18_00:16:59
  host      : 192-222-53-214
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 14084)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-12-18_00:16:59
  host      : 192-222-53-214
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 14085)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2025-12-18_00:16:59
  host      : 192-222-53-214
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 14086)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2025-12-18_00:16:59
  host      : 192-222-53-214
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 14087)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-18_00:16:59
  host      : 192-222-53-214
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 14082)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

      âŒ Failed: W1218 00:16:48.309000 14002 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] 
W1218 00:16:48.309000 14002 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] *****************************************
W1218 00:16:48.309000 14002 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1218 00:16:48.309000 14002 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py:793] *****************************************
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
    generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
        generate(args)generate(args)

  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 626, in <module>
        torch.cuda.set_device(local_rank)torch.cuda.set_device(local_rank)

  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
    generate(args)
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py", line 361, in generate
        torch._C._cuda_setDevice(device)torch._C._cuda_setDevice(device)

RuntimeErrorRuntimeError: : CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


        torch.cuda.set_device(local_rank)torch.cuda.set_device(local_rank)

  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
    torch.cuda.set_device(local_rank)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/cuda/__init__.py", line 478, in set_device
            torch._C._cuda_setDevice(device)torch._C._cuda_setDevice(device)torch._C._cuda_setDevice(device)


RuntimeErrorRuntimeError: : RuntimeErrorCUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
: 

CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

W1218 00:16:59.997000 14002 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 14080 closing signal SIGTERM
W1218 00:16:59.997000 14002 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 14081 closing signal SIGTERM
E1218 00:17:00.161000 14002 /lambda/nfs/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 14082) of binary: /home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/bin/python3
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/Hokin/VMEvalKit/envs/morphic-frames-to-video/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/ubuntu/Hokin/VMEvalKit/submodules/morphic-frames-to-video/generate.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-12-18_00:16:59
  host      : 192-222-53-214
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 14083)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-12-18_00:16:59
  host      : 192-222-53-214
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 14084)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-12-18_00:16:59
  host      : 192-222-53-214
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 14085)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2025-12-18_00:16:59
  host      : 192-222-53-214
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 14086)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2025-12-18_00:16:59
  host      : 192-222-53-214
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 14087)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-18_00:16:59
  host      : 192-222-53-214
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 14082)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================


  ğŸ“Š Model Morphic Summary: 0 completed, 2 failed, 0 skipped in 0h 0m 44s

â±ï¸  Sequential execution completed in 0h 0m 44s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 2 (100.0%)
   Skipped: 0 (0.0%)
   â±ï¸ Duration: 0h 0m 44s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 2 failed | â­ï¸  0 skipped

ğŸ¤– Results by Model:
   morphic-frames-to-video: âœ… 0 | âŒ 2 | â­ï¸  0

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âŒ morphic-frames-to-video: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: hunyuan-video-i2v
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating hunyuan-video-i2v... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): hunyuan-video-i2v

ğŸ” Verifying 1 model(s) for testing...
   âœ… hunyuan-video-i2v: HunyuanVideo
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - hunyuan-video-i2v
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: HunyuanVideo (hunyuan-video-i2v)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with hunyuan-video-i2v
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: hunyuan-video-i2v (will be reused for all tasks)
Warning: HunyuanVideo requires 60-80GB GPU memory for 720p generation

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/hunyuan-video-i2v/tests_task/tests_0001/hunyuan-video-i2v_tests_0001_20251218_001701
   - Video: data/outputs/pilot_experiment/hunyuan-video-i2v/tests_task/tests_0001/hunyuan-video-i2v_tests_0001_20251218_001701/video/
   - Question data: data/outputs/pilot_experiment/hunyuan-video-i2v/tests_task/tests_0001/hunyuan-video-i2v_tests_0001_20251218_001701/question/
   - Metadata: data/outputs/pilot_experiment/hunyuan-video-i2v/tests_task/tests_0001/hunyuan-video-i2v_tests_0001_20251218_001701/metadata.json
     âŒ Failed: The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.

0it [00:00, ?it/s]
0it [00:00, ?it/s]
df: /home/ubuntu/.triton/autotune: No such file or directory
/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/torch/utils/cpp_extension.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging  # type: ignore[attr-defined]
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/utils/import_utils.py", line 853, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/loaders/lora_pipeline.py", line 36, in <module>
    from .lora_base import LoraBaseMixin
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/loaders/lora_base.py", line 47, in <module>
    from peft.tuners.tuners_utils import BaseTunerLayer
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/peft/__init__.py", line 17, in <module>
    from .auto import (
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/peft/auto.py", line 32, in <module>
    from .peft_model import (
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/peft/peft_model.py", line 37, in <module>
    from transformers import Cache, DynamicCache, EncoderDecoderCache, PreTrainedModel
ImportError: cannot import name 'EncoderDecoderCache' from 'transformers' (/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/transformers/__init__.py)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/sample_image2video.py", line 9, in <module>
    from hyvideo.inference import HunyuanVideoSampler
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/inference.py", line 20, in <module>
    from hyvideo.diffusion.schedulers import FlowMatchDiscreteScheduler
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/diffusion/__init__.py", line 1, in <module>
    from .pipelines import HunyuanVideoPipeline
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/diffusion/pipelines/__init__.py", line 1, in <module>
    from .pipeline_hunyuan_video import HunyuanVideoPipeline
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/diffusion/pipelines/pipeline_hunyuan_video.py", line 30, in <module>
    from diffusers.loaders import LoraLoaderMixin, TextualInversionLoaderMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/utils/import_utils.py", line 843, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/utils/import_utils.py", line 855, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import diffusers.loaders.lora_pipeline because of the following error (look up to see its traceback):
cannot import name 'EncoderDecoderCache' from 'transformers' (/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/transformers/__init__.py)

      âŒ Failed: The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.

0it [00:00, ?it/s]
0it [00:00, ?it/s]
df: /home/ubuntu/.triton/autotune: No such file or directory
/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/torch/utils/cpp_extension.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging  # type: ignore[attr-defined]
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/utils/import_utils.py", line 853, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/loaders/lora_pipeline.py", line 36, in <module>
    from .lora_base import LoraBaseMixin
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/loaders/lora_base.py", line 47, in <module>
    from peft.tuners.tuners_utils import BaseTunerLayer
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/peft/__init__.py", line 17, in <module>
    from .auto import (
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/peft/auto.py", line 32, in <module>
    from .peft_model import (
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/peft/peft_model.py", line 37, in <module>
    from transformers import Cache, DynamicCache, EncoderDecoderCache, PreTrainedModel
ImportError: cannot import name 'EncoderDecoderCache' from 'transformers' (/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/transformers/__init__.py)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/sample_image2video.py", line 9, in <module>
    from hyvideo.inference import HunyuanVideoSampler
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/inference.py", line 20, in <module>
    from hyvideo.diffusion.schedulers import FlowMatchDiscreteScheduler
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/diffusion/__init__.py", line 1, in <module>
    from .pipelines import HunyuanVideoPipeline
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/diffusion/pipelines/__init__.py", line 1, in <module>
    from .pipeline_hunyuan_video import HunyuanVideoPipeline
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/diffusion/pipelines/pipeline_hunyuan_video.py", line 30, in <module>
    from diffusers.loaders import LoraLoaderMixin, TextualInversionLoaderMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/utils/import_utils.py", line 843, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/utils/import_utils.py", line 855, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import diffusers.loaders.lora_pipeline because of the following error (look up to see its traceback):
cannot import name 'EncoderDecoderCache' from 'transformers' (/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/transformers/__init__.py)

    [2/2] Processing: tests_0002

  ğŸ¬ Generating: tests_0002 with hunyuan-video-i2v
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0002/first_frame.png
     Prompt: The image shows a horizontally flipped clock. After 2 hours passes on the origin...
Warning: HunyuanVideo requires 60-80GB GPU memory for 720p generation

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/hunyuan-video-i2v/tests_task/tests_0002/hunyuan-video-i2v_tests_0002_20251218_001721
   - Video: data/outputs/pilot_experiment/hunyuan-video-i2v/tests_task/tests_0002/hunyuan-video-i2v_tests_0002_20251218_001721/video/
   - Question data: data/outputs/pilot_experiment/hunyuan-video-i2v/tests_task/tests_0002/hunyuan-video-i2v_tests_0002_20251218_001721/question/
   - Metadata: data/outputs/pilot_experiment/hunyuan-video-i2v/tests_task/tests_0002/hunyuan-video-i2v_tests_0002_20251218_001721/metadata.json
     âŒ Failed: /home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/torch/utils/cpp_extension.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging  # type: ignore[attr-defined]
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/utils/import_utils.py", line 853, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/loaders/lora_pipeline.py", line 36, in <module>
    from .lora_base import LoraBaseMixin
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/loaders/lora_base.py", line 47, in <module>
    from peft.tuners.tuners_utils import BaseTunerLayer
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/peft/__init__.py", line 17, in <module>
    from .auto import (
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/peft/auto.py", line 32, in <module>
    from .peft_model import (
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/peft/peft_model.py", line 37, in <module>
    from transformers import Cache, DynamicCache, EncoderDecoderCache, PreTrainedModel
ImportError: cannot import name 'EncoderDecoderCache' from 'transformers' (/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/transformers/__init__.py)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/sample_image2video.py", line 9, in <module>
    from hyvideo.inference import HunyuanVideoSampler
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/inference.py", line 20, in <module>
    from hyvideo.diffusion.schedulers import FlowMatchDiscreteScheduler
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/diffusion/__init__.py", line 1, in <module>
    from .pipelines import HunyuanVideoPipeline
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/diffusion/pipelines/__init__.py", line 1, in <module>
    from .pipeline_hunyuan_video import HunyuanVideoPipeline
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/diffusion/pipelines/pipeline_hunyuan_video.py", line 30, in <module>
    from diffusers.loaders import LoraLoaderMixin, TextualInversionLoaderMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/utils/import_utils.py", line 843, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/utils/import_utils.py", line 855, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import diffusers.loaders.lora_pipeline because of the following error (look up to see its traceback):
cannot import name 'EncoderDecoderCache' from 'transformers' (/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/transformers/__init__.py)

      âŒ Failed: /home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/torch/utils/cpp_extension.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging  # type: ignore[attr-defined]
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/utils/import_utils.py", line 853, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/loaders/lora_pipeline.py", line 36, in <module>
    from .lora_base import LoraBaseMixin
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/loaders/lora_base.py", line 47, in <module>
    from peft.tuners.tuners_utils import BaseTunerLayer
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/peft/__init__.py", line 17, in <module>
    from .auto import (
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/peft/auto.py", line 32, in <module>
    from .peft_model import (
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/peft/peft_model.py", line 37, in <module>
    from transformers import Cache, DynamicCache, EncoderDecoderCache, PreTrainedModel
ImportError: cannot import name 'EncoderDecoderCache' from 'transformers' (/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/transformers/__init__.py)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/sample_image2video.py", line 9, in <module>
    from hyvideo.inference import HunyuanVideoSampler
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/inference.py", line 20, in <module>
    from hyvideo.diffusion.schedulers import FlowMatchDiscreteScheduler
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/diffusion/__init__.py", line 1, in <module>
    from .pipelines import HunyuanVideoPipeline
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/diffusion/pipelines/__init__.py", line 1, in <module>
    from .pipeline_hunyuan_video import HunyuanVideoPipeline
  File "/lambda/nfs/Hokin/VMEvalKit/submodules/HunyuanVideo-I2V/hyvideo/diffusion/pipelines/pipeline_hunyuan_video.py", line 30, in <module>
    from diffusers.loaders import LoraLoaderMixin, TextualInversionLoaderMixin
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/utils/import_utils.py", line 843, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/diffusers/utils/import_utils.py", line 855, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import diffusers.loaders.lora_pipeline because of the following error (look up to see its traceback):
cannot import name 'EncoderDecoderCache' from 'transformers' (/home/ubuntu/Hokin/VMEvalKit/envs/hunyuan-video-i2v/lib/python3.10/site-packages/transformers/__init__.py)


  ğŸ“Š Model HunyuanVideo Summary: 0 completed, 2 failed, 0 skipped in 0h 0m 30s

â±ï¸  Sequential execution completed in 0h 0m 30s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 2 (100.0%)
   Skipped: 0 (0.0%)
   â±ï¸ Duration: 0h 0m 30s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 2 failed | â­ï¸  0 skipped

ğŸ¤– Results by Model:
   hunyuan-video-i2v: âœ… 0 | âŒ 2 | â­ï¸  0

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âŒ hunyuan-video-i2v: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: dynamicrafter-256
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating dynamicrafter-256... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): dynamicrafter-256

ğŸ” Verifying 1 model(s) for testing...
   âœ… dynamicrafter-256: DynamiCrafter
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - dynamicrafter-256
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: DynamiCrafter (dynamicrafter-256)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with dynamicrafter-256
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: dynamicrafter-256 (will be reused for all tasks)
DEBUG - Return code: 0
DEBUG - stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully

DEBUG - stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-256/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@18.612] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:

OpenCV(4.8.1) /io/opencv/modules/videoio/src/cap_images.cpp:267: error: (-215:Assertion failed) number < max_number in function 'icvExtractPattern'




âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/dynamicrafter-256/tests_task/tests_0001/dynamicrafter-256_tests_0001_20251218_001732
   - Video: data/outputs/pilot_experiment/dynamicrafter-256/tests_task/tests_0001/dynamicrafter-256_tests_0001_20251218_001732/video/
   - Question data: data/outputs/pilot_experiment/dynamicrafter-256/tests_task/tests_0001/dynamicrafter-256_tests_0001_20251218_001732/question/
   - Metadata: data/outputs/pilot_experiment/dynamicrafter-256/tests_task/tests_0001/dynamicrafter-256_tests_0001_20251218_001732/metadata.json
     âŒ Failed: DynamiCrafter inference failed. stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully
, stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-256/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@18.612] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:

      âŒ Failed: DynamiCrafter inference failed. stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully
, stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-256/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@18.612] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:

    [2/2] Processing: tests_0002

  ğŸ¬ Generating: tests_0002 with dynamicrafter-256
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0002/first_frame.png
     Prompt: The image shows a horizontally flipped clock. After 2 hours passes on the origin...
DEBUG - Return code: 0
DEBUG - stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully

DEBUG - stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-256/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@13.010] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:

OpenCV(4.8.1) /io/opencv/modules/videoio/src/cap_images.cpp:267: error: (-215:Assertion failed) number < max_number in function 'icvExtractPattern'




âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/dynamicrafter-256/tests_task/tests_0002/dynamicrafter-256_tests_0002_20251218_001800
   - Video: data/outputs/pilot_experiment/dynamicrafter-256/tests_task/tests_0002/dynamicrafter-256_tests_0002_20251218_001800/video/
   - Question data: data/outputs/pilot_experiment/dynamicrafter-256/tests_task/tests_0002/dynamicrafter-256_tests_0002_20251218_001800/question/
   - Metadata: data/outputs/pilot_experiment/dynamicrafter-256/tests_task/tests_0002/dynamicrafter-256_tests_0002_20251218_001800/metadata.json
     âŒ Failed: DynamiCrafter inference failed. stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully
, stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-256/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@13.010] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:

      âŒ Failed: DynamiCrafter inference failed. stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully
, stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-256/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@13.010] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:


  ğŸ“Š Model DynamiCrafter Summary: 0 completed, 2 failed, 0 skipped in 0h 0m 44s

â±ï¸  Sequential execution completed in 0h 0m 44s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 2 (100.0%)
   Skipped: 0 (0.0%)
   â±ï¸ Duration: 0h 0m 44s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 2 failed | â­ï¸  0 skipped

ğŸ¤– Results by Model:
   dynamicrafter-256: âœ… 0 | âŒ 2 | â­ï¸  0

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âŒ dynamicrafter-256: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: dynamicrafter-512
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating dynamicrafter-512... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): dynamicrafter-512

ğŸ” Verifying 1 model(s) for testing...
   âœ… dynamicrafter-512: DynamiCrafter
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - dynamicrafter-512
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: DynamiCrafter (dynamicrafter-512)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with dynamicrafter-512
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: dynamicrafter-512 (will be reused for all tasks)
DEBUG - Return code: 0
DEBUG - stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully

DEBUG - stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-512/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@17.425] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:

OpenCV(4.8.1) /io/opencv/modules/videoio/src/cap_images.cpp:267: error: (-215:Assertion failed) number < max_number in function 'icvExtractPattern'




âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/dynamicrafter-512/tests_task/tests_0001/dynamicrafter-512_tests_0001_20251218_001816
   - Video: data/outputs/pilot_experiment/dynamicrafter-512/tests_task/tests_0001/dynamicrafter-512_tests_0001_20251218_001816/video/
   - Question data: data/outputs/pilot_experiment/dynamicrafter-512/tests_task/tests_0001/dynamicrafter-512_tests_0001_20251218_001816/question/
   - Metadata: data/outputs/pilot_experiment/dynamicrafter-512/tests_task/tests_0001/dynamicrafter-512_tests_0001_20251218_001816/metadata.json
     âŒ Failed: DynamiCrafter inference failed. stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully
, stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-512/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@17.425] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:

      âŒ Failed: DynamiCrafter inference failed. stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully
, stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-512/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@17.425] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:

    [2/2] Processing: tests_0002

  ğŸ¬ Generating: tests_0002 with dynamicrafter-512
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0002/first_frame.png
     Prompt: The image shows a horizontally flipped clock. After 2 hours passes on the origin...
DEBUG - Return code: 0
DEBUG - stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully

DEBUG - stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-512/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@13.085] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:

OpenCV(4.8.1) /io/opencv/modules/videoio/src/cap_images.cpp:267: error: (-215:Assertion failed) number < max_number in function 'icvExtractPattern'




âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/dynamicrafter-512/tests_task/tests_0002/dynamicrafter-512_tests_0002_20251218_001843
   - Video: data/outputs/pilot_experiment/dynamicrafter-512/tests_task/tests_0002/dynamicrafter-512_tests_0002_20251218_001843/video/
   - Question data: data/outputs/pilot_experiment/dynamicrafter-512/tests_task/tests_0002/dynamicrafter-512_tests_0002_20251218_001843/question/
   - Metadata: data/outputs/pilot_experiment/dynamicrafter-512/tests_task/tests_0002/dynamicrafter-512_tests_0002_20251218_001843/metadata.json
     âŒ Failed: DynamiCrafter inference failed. stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully
, stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-512/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@13.085] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:

      âŒ Failed: DynamiCrafter inference failed. stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully
, stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-512/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@13.085] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:


  ğŸ“Š Model DynamiCrafter Summary: 0 completed, 2 failed, 0 skipped in 0h 0m 43s

â±ï¸  Sequential execution completed in 0h 0m 43s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 2 (100.0%)
   Skipped: 0 (0.0%)
   â±ï¸ Duration: 0h 0m 43s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 2 failed | â­ï¸  0 skipped

ğŸ¤– Results by Model:
   dynamicrafter-512: âœ… 0 | âŒ 2 | â­ï¸  0

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âŒ dynamicrafter-512: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: dynamicrafter-1024
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating dynamicrafter-1024... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): dynamicrafter-1024

ğŸ” Verifying 1 model(s) for testing...
   âœ… dynamicrafter-1024: DynamiCrafter
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - dynamicrafter-1024
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: DynamiCrafter (dynamicrafter-1024)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with dynamicrafter-1024
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: dynamicrafter-1024 (will be reused for all tasks)
DEBUG - Return code: 0
DEBUG - stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully

DEBUG - stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-1024/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@17.361] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:

OpenCV(4.8.1) /io/opencv/modules/videoio/src/cap_images.cpp:267: error: (-215:Assertion failed) number < max_number in function 'icvExtractPattern'




âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/dynamicrafter-1024/tests_task/tests_0001/dynamicrafter-1024_tests_0001_20251218_001900
   - Video: data/outputs/pilot_experiment/dynamicrafter-1024/tests_task/tests_0001/dynamicrafter-1024_tests_0001_20251218_001900/video/
   - Question data: data/outputs/pilot_experiment/dynamicrafter-1024/tests_task/tests_0001/dynamicrafter-1024_tests_0001_20251218_001900/question/
   - Metadata: data/outputs/pilot_experiment/dynamicrafter-1024/tests_task/tests_0001/dynamicrafter-1024_tests_0001_20251218_001900/metadata.json
     âŒ Failed: DynamiCrafter inference failed. stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully
, stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-1024/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@17.361] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:
      âŒ Failed: DynamiCrafter inference failed. stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully
, stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-1024/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@17.361] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:
    [2/2] Processing: tests_0002

  ğŸ¬ Generating: tests_0002 with dynamicrafter-1024
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0002/first_frame.png
     Prompt: The image shows a horizontally flipped clock. After 2 hours passes on the origin...
DEBUG - Return code: 0
DEBUG - stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully

DEBUG - stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-1024/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@13.036] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:

OpenCV(4.8.1) /io/opencv/modules/videoio/src/cap_images.cpp:267: error: (-215:Assertion failed) number < max_number in function 'icvExtractPattern'




âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/dynamicrafter-1024/tests_task/tests_0002/dynamicrafter-1024_tests_0002_20251218_001927
   - Video: data/outputs/pilot_experiment/dynamicrafter-1024/tests_task/tests_0002/dynamicrafter-1024_tests_0002_20251218_001927/video/
   - Question data: data/outputs/pilot_experiment/dynamicrafter-1024/tests_task/tests_0002/dynamicrafter-1024_tests_0002_20251218_001927/question/
   - Metadata: data/outputs/pilot_experiment/dynamicrafter-1024/tests_task/tests_0002/dynamicrafter-1024_tests_0002_20251218_001927/metadata.json
     âŒ Failed: DynamiCrafter inference failed. stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully
, stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-1024/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@13.036] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:
      âŒ Failed: DynamiCrafter inference failed. stdout: AE working on z of shape (1, 4, 32, 32) = 4096 dimensions.
Failed to load model, using placeholder: No module named 'open_clip'
Video generation completed successfully
, stderr: /home/ubuntu/Hokin/VMEvalKit/envs/dynamicrafter-1024/lib/python3.10/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)
[ERROR:0@13.036] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:

  ğŸ“Š Model DynamiCrafter Summary: 0 completed, 2 failed, 0 skipped in 0h 0m 43s

â±ï¸  Sequential execution completed in 0h 0m 43s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 2 (100.0%)
   Skipped: 0 (0.0%)
   â±ï¸ Duration: 0h 0m 43s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 2 failed | â­ï¸  0 skipped

ğŸ¤– Results by Model:
   dynamicrafter-1024: âœ… 0 | âŒ 2 | â­ï¸  0

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âŒ dynamicrafter-1024: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: videocrafter2-512
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating videocrafter2-512... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): videocrafter2-512

ğŸ” Verifying 1 model(s) for testing...
   âœ… videocrafter2-512: VideoCrafter
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - videocrafter2-512
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: VideoCrafter (videocrafter2-512)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with videocrafter2-512
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: videocrafter2-512 (will be reused for all tasks)

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/videocrafter2-512/tests_task/tests_0001/videocrafter2-512_tests_0001_20251218_001943
   - Video: data/outputs/pilot_experiment/videocrafter2-512/tests_task/tests_0001/videocrafter2-512_tests_0001_20251218_001943/video/
   - Question data: data/outputs/pilot_experiment/videocrafter2-512/tests_task/tests_0001/videocrafter2-512_tests_0001_20251218_001943/question/
   - Metadata: data/outputs/pilot_experiment/videocrafter2-512/tests_task/tests_0001/videocrafter2-512_tests_0001_20251218_001943/metadata.json
     âŒ Failed: VideoCrafter inference requires manual setup of model checkpoints. Please refer to the VideoCrafter repository for setup instructions. Original error: /home/ubuntu/Hokin/VMEvalKit/envs/videocrafter2-512/lib/python3.10/site-packages/lightning_fabric/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)

      âŒ Failed: VideoCrafter inference requires manual setup of model checkpoints. Please refer to the VideoCrafter repository for setup instructions. Original error: /home/ubuntu/Hokin/VMEvalKit/envs/videocrafter2-512/lib/python3.10/site-packages/lightning_fabric/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)

    [2/2] Processing: tests_0002

  ğŸ¬ Generating: tests_0002 with videocrafter2-512
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0002/first_frame.png
     Prompt: The image shows a horizontally flipped clock. After 2 hours passes on the origin...

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/videocrafter2-512/tests_task/tests_0002/videocrafter2-512_tests_0002_20251218_002005
   - Video: data/outputs/pilot_experiment/videocrafter2-512/tests_task/tests_0002/videocrafter2-512_tests_0002_20251218_002005/video/
   - Question data: data/outputs/pilot_experiment/videocrafter2-512/tests_task/tests_0002/videocrafter2-512_tests_0002_20251218_002005/question/
   - Metadata: data/outputs/pilot_experiment/videocrafter2-512/tests_task/tests_0002/videocrafter2-512_tests_0002_20251218_002005/metadata.json
     âŒ Failed: VideoCrafter inference requires manual setup of model checkpoints. Please refer to the VideoCrafter repository for setup instructions. Original error: /home/ubuntu/Hokin/VMEvalKit/envs/videocrafter2-512/lib/python3.10/site-packages/lightning_fabric/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)

      âŒ Failed: VideoCrafter inference requires manual setup of model checkpoints. Please refer to the VideoCrafter repository for setup instructions. Original error: /home/ubuntu/Hokin/VMEvalKit/envs/videocrafter2-512/lib/python3.10/site-packages/lightning_fabric/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  __import__("pkg_resources").declare_namespace(__name__)


  ğŸ“Š Model VideoCrafter Summary: 0 completed, 2 failed, 0 skipped in 0h 0m 36s

â±ï¸  Sequential execution completed in 0h 0m 36s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 0 (0.0%)
   Failed: 2 (100.0%)
   Skipped: 0 (0.0%)
   â±ï¸ Duration: 0h 0m 36s

ğŸ¯ Results by Domain:
   Tests: âœ… 0 completed | âŒ 2 failed | â­ï¸  0 skipped

ğŸ¤– Results by Model:
   videocrafter2-512: âœ… 0 | âŒ 2 | â­ï¸  0

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âŒ videocrafter2-512: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: cogvideox-5b-i2v
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating cogvideox-5b-i2v... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): cogvideox-5b-i2v

ğŸ” Verifying 1 model(s) for testing...
   âœ… cogvideox-5b-i2v: CogVideoX
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - cogvideox-5b-i2v
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: CogVideoX (cogvideox-5b-i2v)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with cogvideox-5b-i2v
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: cogvideox-5b-i2v (will be reused for all tasks)
Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]Fetching 17 files:  12%|â–ˆâ–        | 2/17 [00:00<00:00, 16.79it/s]Fetching 17 files:  24%|â–ˆâ–ˆâ–       | 4/17 [00:04<00:17,  1.34s/it]Fetching 17 files:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:08<00:25,  2.14s/it]Fetching 17 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:08<00:00,  1.97it/s]
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:03,  1.08it/s]Loading pipeline components...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:01<00:01,  2.05it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][A
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  8.26it/s][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.52it/s][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.47it/s]
Loading pipeline components...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:01<00:00,  2.50it/s]Loading pipeline components...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:01<00:00,  3.53it/s]The config attributes {'invert_scale_latents': False} were passed to AutoencoderKLCogVideoX, but are not expected and will be ignored. Please verify your config.json configuration file.
Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  4.49it/s]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.08it/s]
  0%|          | 0/50 [00:00<?, ?it/s]  2%|â–         | 1/50 [00:04<03:22,  4.12s/it]  4%|â–         | 2/50 [00:07<03:01,  3.78s/it]  6%|â–Œ         | 3/50 [00:11<02:52,  3.67s/it]  8%|â–Š         | 4/50 [00:14<02:46,  3.62s/it] 10%|â–ˆ         | 5/50 [00:18<02:42,  3.61s/it] 12%|â–ˆâ–        | 6/50 [00:21<02:37,  3.58s/it] 14%|â–ˆâ–        | 7/50 [00:25<02:33,  3.58s/it] 16%|â–ˆâ–Œ        | 8/50 [00:29<02:31,  3.61s/it] 18%|â–ˆâ–Š        | 9/50 [00:32<02:26,  3.56s/it] 20%|â–ˆâ–ˆ        | 10/50 [00:36<02:21,  3.54s/it] 22%|â–ˆâ–ˆâ–       | 11/50 [00:39<02:19,  3.58s/it] 24%|â–ˆâ–ˆâ–       | 12/50 [00:43<02:14,  3.55s/it] 26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:46<02:10,  3.53s/it] 28%|â–ˆâ–ˆâ–Š       | 14/50 [00:50<02:06,  3.52s/it] 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:53<02:04,  3.56s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:57<02:00,  3.54s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [01:00<01:56,  3.53s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [01:04<01:52,  3.51s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [01:07<01:48,  3.51s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [01:11<01:46,  3.56s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [01:14<01:42,  3.54s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [01:18<01:38,  3.53s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [01:21<01:34,  3.51s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [01:25<01:32,  3.56s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [01:29<01:28,  3.54s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [01:32<01:24,  3.52s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [01:36<01:20,  3.51s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [01:39<01:17,  3.51s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [01:43<01:14,  3.56s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [01:46<01:10,  3.54s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [01:50<01:06,  3.52s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [01:53<01:03,  3.51s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [01:57<01:00,  3.56s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [02:00<00:56,  3.53s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [02:04<00:52,  3.52s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [02:07<00:49,  3.51s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [02:11<00:45,  3.51s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [02:15<00:42,  3.56s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [02:18<00:38,  3.54s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [02:21<00:35,  3.52s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [02:25<00:31,  3.51s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [02:29<00:28,  3.56s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [02:32<00:24,  3.54s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [02:36<00:21,  3.52s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [02:39<00:17,  3.51s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [02:43<00:14,  3.51s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [02:46<00:10,  3.56s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [02:50<00:07,  3.54s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [02:53<00:03,  3.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:57<00:00,  3.51s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:57<00:00,  3.54s/it]
It is recommended to use `export_to_video` with `imageio` and `imageio-ffmpeg` as a backend. 
These libraries are not present in your environment. Attempting to use legacy OpenCV backend to export video. 
Support for the OpenCV backend will be deprecated in a future Diffusers version

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/cogvideox-5b-i2v/tests_task/tests_0001/cogvideox-5b-i2v_tests_0001_20251218_002020
   - Video: data/outputs/pilot_experiment/cogvideox-5b-i2v/tests_task/tests_0001/cogvideox-5b-i2v_tests_0001_20251218_002020/video/
   - Question data: data/outputs/pilot_experiment/cogvideox-5b-i2v/tests_task/tests_0001/cogvideox-5b-i2v_tests_0001_20251218_002020/question/
   - Metadata: data/outputs/pilot_experiment/cogvideox-5b-i2v/tests_task/tests_0001/cogvideox-5b-i2v_tests_0001_20251218_002020/metadata.json
     âœ… Success! Structured output saved to: data/outputs/pilot_experiment/cogvideox-5b-i2v/tests_task/tests_0001/cogvideox-5b-i2v_tests_0001_20251218_002020
      âœ… Completed successfully
    [2/2] Processing: tests_0002

  ğŸ¬ Generating: tests_0002 with cogvideox-5b-i2v
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0002/first_frame.png
     Prompt: The image shows a horizontally flipped clock. After 2 hours passes on the origin...
  0%|          | 0/50 [00:00<?, ?it/s]  2%|â–         | 1/50 [00:03<02:50,  3.49s/it]  4%|â–         | 2/50 [00:07<02:48,  3.51s/it]  6%|â–Œ         | 3/50 [00:10<02:45,  3.52s/it]  8%|â–Š         | 4/50 [00:14<02:42,  3.52s/it] 10%|â–ˆ         | 5/50 [00:17<02:39,  3.54s/it] 12%|â–ˆâ–        | 6/50 [00:21<02:35,  3.54s/it] 14%|â–ˆâ–        | 7/50 [00:24<02:32,  3.55s/it] 16%|â–ˆâ–Œ        | 8/50 [00:28<02:30,  3.59s/it] 18%|â–ˆâ–Š        | 9/50 [00:31<02:25,  3.55s/it] 20%|â–ˆâ–ˆ        | 10/50 [00:35<02:21,  3.54s/it] 22%|â–ˆâ–ˆâ–       | 11/50 [00:39<02:19,  3.58s/it] 24%|â–ˆâ–ˆâ–       | 12/50 [00:42<02:14,  3.55s/it] 26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:46<02:10,  3.53s/it] 28%|â–ˆâ–ˆâ–Š       | 14/50 [00:49<02:06,  3.51s/it] 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:53<02:04,  3.56s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:56<02:00,  3.53s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [01:00<01:56,  3.52s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [01:03<01:52,  3.51s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [01:07<01:48,  3.51s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [01:10<01:46,  3.55s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [01:14<01:42,  3.53s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [01:17<01:38,  3.52s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [01:21<01:34,  3.50s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [01:24<01:32,  3.55s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [01:28<01:28,  3.53s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [01:31<01:24,  3.52s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [01:35<01:20,  3.51s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [01:38<01:17,  3.50s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [01:42<01:14,  3.55s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [01:45<01:10,  3.53s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [01:49<01:06,  3.52s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [01:52<01:03,  3.50s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [01:56<01:00,  3.56s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [02:00<00:56,  3.53s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [02:03<00:52,  3.52s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [02:07<00:49,  3.51s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [02:10<00:45,  3.50s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [02:14<00:42,  3.55s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [02:17<00:38,  3.53s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [02:21<00:35,  3.52s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [02:24<00:31,  3.50s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [02:28<00:28,  3.55s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [02:31<00:24,  3.53s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [02:35<00:21,  3.52s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [02:38<00:17,  3.51s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [02:42<00:14,  3.50s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [02:45<00:10,  3.55s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [02:49<00:07,  3.53s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [02:52<00:03,  3.52s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:56<00:00,  3.50s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:56<00:00,  3.53s/it]
It is recommended to use `export_to_video` with `imageio` and `imageio-ffmpeg` as a backend. 
These libraries are not present in your environment. Attempting to use legacy OpenCV backend to export video. 
Support for the OpenCV backend will be deprecated in a future Diffusers version

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/cogvideox-5b-i2v/tests_task/tests_0002/cogvideox-5b-i2v_tests_0002_20251218_002357
   - Video: data/outputs/pilot_experiment/cogvideox-5b-i2v/tests_task/tests_0002/cogvideox-5b-i2v_tests_0002_20251218_002357/video/
   - Question data: data/outputs/pilot_experiment/cogvideox-5b-i2v/tests_task/tests_0002/cogvideox-5b-i2v_tests_0002_20251218_002357/question/
   - Metadata: data/outputs/pilot_experiment/cogvideox-5b-i2v/tests_task/tests_0002/cogvideox-5b-i2v_tests_0002_20251218_002357/metadata.json
     âœ… Success! Structured output saved to: data/outputs/pilot_experiment/cogvideox-5b-i2v/tests_task/tests_0002/cogvideox-5b-i2v_tests_0002_20251218_002357
      âœ… Completed successfully

  ğŸ“Š Model CogVideoX Summary: 2 completed, 0 failed, 0 skipped in 0h 6m 44s

â±ï¸  Sequential execution completed in 0h 6m 44s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 2 (100.0%)
   Failed: 0 (0.0%)
   Skipped: 0 (0.0%)
   â±ï¸ Duration: 0h 6m 44s

ğŸ¯ Results by Domain:
   Tests: âœ… 2 completed | âŒ 0 failed | â­ï¸  0 skipped

ğŸ¤– Results by Model:
   cogvideox-5b-i2v: âœ… 2 | âŒ 0 | â­ï¸  0

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âœ… cogvideox-5b-i2v: 2 videos generated âœ“

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: cogvideox1.5-5b-i2v
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating cogvideox1.5-5b-i2v... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): cogvideox1.5-5b-i2v

ğŸ” Verifying 1 model(s) for testing...
   âœ… cogvideox1.5-5b-i2v: CogVideoX
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - cogvideox1.5-5b-i2v
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: CogVideoX (cogvideox1.5-5b-i2v)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with cogvideox1.5-5b-i2v
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: cogvideox1.5-5b-i2v (will be reused for all tasks)
Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]Fetching 19 files:  11%|â–ˆ         | 2/19 [00:00<00:00, 17.45it/s]Fetching 19 files:  21%|â–ˆâ–ˆ        | 4/19 [00:10<00:45,  3.04s/it]Fetching 19 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:10<00:00,  1.83it/s]
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]Loading pipeline components...:  20%|â–ˆâ–ˆ        | 1/5 [00:00<00:01,  3.83it/s]Loading pipeline components...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00,  5.77it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  3.04it/s][A
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  3.00it/s][A
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  3.01it/s][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  3.15it/s][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  3.09it/s]
Loading pipeline components...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:01<00:01,  1.40it/s]Loading pipeline components...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:02<00:00,  1.63it/s]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.21it/s]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.12it/s]
  0%|          | 0/50 [00:00<?, ?it/s]  2%|â–         | 1/50 [00:06<05:20,  6.55s/it]  4%|â–         | 2/50 [00:12<04:56,  6.18s/it]  6%|â–Œ         | 3/50 [00:18<04:45,  6.07s/it]  8%|â–Š         | 4/50 [00:24<04:36,  6.02s/it] 10%|â–ˆ         | 5/50 [00:30<04:29,  5.99s/it] 12%|â–ˆâ–        | 6/50 [00:36<04:22,  5.97s/it] 14%|â–ˆâ–        | 7/50 [00:42<04:16,  5.96s/it] 16%|â–ˆâ–Œ        | 8/50 [00:48<04:10,  5.96s/it] 18%|â–ˆâ–Š        | 9/50 [00:54<04:04,  5.96s/it] 20%|â–ˆâ–ˆ        | 10/50 [01:00<03:58,  5.96s/it] 22%|â–ˆâ–ˆâ–       | 11/50 [01:05<03:52,  5.96s/it] 24%|â–ˆâ–ˆâ–       | 12/50 [01:11<03:46,  5.96s/it] 26%|â–ˆâ–ˆâ–Œ       | 13/50 [01:17<03:40,  5.95s/it] 28%|â–ˆâ–ˆâ–Š       | 14/50 [01:23<03:34,  5.95s/it] 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [01:29<03:28,  5.95s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [01:35<03:22,  5.95s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [01:41<03:16,  5.95s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [01:47<03:10,  5.95s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [01:53<03:04,  5.95s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [01:59<02:58,  5.95s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [02:05<02:52,  5.95s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [02:11<02:46,  5.95s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [02:17<02:40,  5.95s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [02:23<02:34,  5.95s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [02:29<02:28,  5.95s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [02:35<02:22,  5.95s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [02:41<02:16,  5.95s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [02:47<02:10,  5.95s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [02:53<02:05,  5.95s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [02:59<01:59,  5.96s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [03:05<01:53,  5.96s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [03:10<01:47,  5.96s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [03:16<01:41,  5.96s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [03:22<01:35,  5.96s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [03:28<01:29,  5.96s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [03:34<01:23,  5.96s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [03:40<01:17,  5.96s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [03:46<01:11,  5.96s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [03:52<01:05,  5.96s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [03:58<00:59,  5.96s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [04:04<00:53,  5.96s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [04:10<00:47,  5.96s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [04:16<00:41,  5.96s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [04:22<00:35,  5.96s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [04:28<00:29,  5.95s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [04:34<00:23,  5.95s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [04:40<00:17,  5.95s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [04:46<00:11,  5.95s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [04:52<00:05,  5.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [04:58<00:00,  5.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [04:58<00:00,  5.96s/it]
It is recommended to use `export_to_video` with `imageio` and `imageio-ffmpeg` as a backend. 
These libraries are not present in your environment. Attempting to use legacy OpenCV backend to export video. 
Support for the OpenCV backend will be deprecated in a future Diffusers version

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/cogvideox1.5-5b-i2v/tests_task/tests_0001/cogvideox1.5-5b-i2v_tests_0001_20251218_002708
   - Video: data/outputs/pilot_experiment/cogvideox1.5-5b-i2v/tests_task/tests_0001/cogvideox1.5-5b-i2v_tests_0001_20251218_002708/video/
   - Question data: data/outputs/pilot_experiment/cogvideox1.5-5b-i2v/tests_task/tests_0001/cogvideox1.5-5b-i2v_tests_0001_20251218_002708/question/
   - Metadata: data/outputs/pilot_experiment/cogvideox1.5-5b-i2v/tests_task/tests_0001/cogvideox1.5-5b-i2v_tests_0001_20251218_002708/metadata.json
     âœ… Success! Structured output saved to: data/outputs/pilot_experiment/cogvideox1.5-5b-i2v/tests_task/tests_0001/cogvideox1.5-5b-i2v_tests_0001_20251218_002708
      âœ… Completed successfully
    [2/2] Processing: tests_0002

  ğŸ¬ Generating: tests_0002 with cogvideox1.5-5b-i2v
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0002/first_frame.png
     Prompt: The image shows a horizontally flipped clock. After 2 hours passes on the origin...
  0%|          | 0/50 [00:00<?, ?it/s]  2%|â–         | 1/50 [00:05<04:50,  5.92s/it]  4%|â–         | 2/50 [00:11<04:44,  5.93s/it]  6%|â–Œ         | 3/50 [00:17<04:38,  5.93s/it]  8%|â–Š         | 4/50 [00:23<04:33,  5.94s/it] 10%|â–ˆ         | 5/50 [00:29<04:27,  5.93s/it] 12%|â–ˆâ–        | 6/50 [00:35<04:21,  5.93s/it] 14%|â–ˆâ–        | 7/50 [00:41<04:15,  5.93s/it] 16%|â–ˆâ–Œ        | 8/50 [00:47<04:09,  5.93s/it] 18%|â–ˆâ–Š        | 9/50 [00:53<04:03,  5.93s/it] 20%|â–ˆâ–ˆ        | 10/50 [00:59<03:57,  5.94s/it] 22%|â–ˆâ–ˆâ–       | 11/50 [01:05<03:51,  5.94s/it] 24%|â–ˆâ–ˆâ–       | 12/50 [01:11<03:45,  5.94s/it] 26%|â–ˆâ–ˆâ–Œ       | 13/50 [01:17<03:40,  5.95s/it] 28%|â–ˆâ–ˆâ–Š       | 14/50 [01:23<03:34,  5.95s/it] 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [01:29<03:28,  5.95s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [01:35<03:22,  5.95s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [01:40<03:16,  5.95s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [01:46<03:10,  5.95s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [01:52<03:04,  5.95s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [01:58<02:58,  5.95s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [02:04<02:52,  5.95s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [02:10<02:46,  5.95s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [02:16<02:40,  5.95s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [02:22<02:34,  5.95s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [02:28<02:28,  5.95s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [02:34<02:22,  5.95s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [02:40<02:16,  5.95s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [02:46<02:10,  5.95s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [02:52<02:04,  5.95s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [02:58<01:59,  5.95s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [03:04<01:53,  5.96s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [03:10<01:47,  5.96s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [03:16<01:41,  5.96s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [03:22<01:35,  5.96s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [03:28<01:29,  5.96s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [03:34<01:23,  5.96s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [03:40<01:17,  5.96s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [03:46<01:11,  5.95s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [03:51<01:05,  5.95s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [03:57<00:59,  5.95s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [04:03<00:53,  5.95s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [04:09<00:47,  5.95s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [04:15<00:41,  5.95s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [04:21<00:35,  5.95s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [04:27<00:29,  5.95s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [04:33<00:23,  5.95s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [04:39<00:17,  5.95s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [04:45<00:11,  5.95s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [04:51<00:05,  5.94s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [04:57<00:00,  5.94s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [04:57<00:00,  5.95s/it]
It is recommended to use `export_to_video` with `imageio` and `imageio-ffmpeg` as a backend. 
These libraries are not present in your environment. Attempting to use legacy OpenCV backend to export video. 
Support for the OpenCV backend will be deprecated in a future Diffusers version

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/cogvideox1.5-5b-i2v/tests_task/tests_0002/cogvideox1.5-5b-i2v_tests_0002_20251218_003303
   - Video: data/outputs/pilot_experiment/cogvideox1.5-5b-i2v/tests_task/tests_0002/cogvideox1.5-5b-i2v_tests_0002_20251218_003303/video/
   - Question data: data/outputs/pilot_experiment/cogvideox1.5-5b-i2v/tests_task/tests_0002/cogvideox1.5-5b-i2v_tests_0002_20251218_003303/question/
   - Metadata: data/outputs/pilot_experiment/cogvideox1.5-5b-i2v/tests_task/tests_0002/cogvideox1.5-5b-i2v_tests_0002_20251218_003303/metadata.json
     âœ… Success! Structured output saved to: data/outputs/pilot_experiment/cogvideox1.5-5b-i2v/tests_task/tests_0002/cogvideox1.5-5b-i2v_tests_0002_20251218_003303
      âœ… Completed successfully

  ğŸ“Š Model CogVideoX Summary: 2 completed, 0 failed, 0 skipped in 0h 11m 21s

â±ï¸  Sequential execution completed in 0h 11m 21s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 2 (100.0%)
   Failed: 0 (0.0%)
   Skipped: 0 (0.0%)
   â±ï¸ Duration: 0h 11m 21s

ğŸ¯ Results by Domain:
   Tests: âœ… 2 completed | âŒ 0 failed | â­ï¸  0 skipped

ğŸ¤– Results by Model:
   cogvideox1.5-5b-i2v: âœ… 2 | âŒ 0 | â­ï¸  0

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âœ… cogvideox1.5-5b-i2v: 2 videos generated âœ“

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: sana-video-2b-480p
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating sana-video-2b-480p... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): sana-video-2b-480p

ğŸ” Verifying 1 model(s) for testing...
   âœ… sana-video-2b-480p: SANA-Video
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - sana-video-2b-480p
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: SANA-Video (sana-video-2b-480p)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with sana-video-2b-480p
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 177, in run
    wrapper_class = _load_model_wrapper(model_name)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 42, in _load_model_wrapper
    module = importlib.import_module(config["wrapper_module"])
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/sana_inference.py", line 83, in <module>
    from diffusers import SanaImageToVideoPipeline
ImportError: cannot import name 'SanaImageToVideoPipeline' from 'diffusers' (/home/ubuntu/Hokin/VMEvalKit/envs/sana-video-2b-480p/lib/python3.10/site-packages/diffusers/__init__.py)
   âŒ sana-video-2b-480p: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: sana-video-2b-longlive
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating sana-video-2b-longlive... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): sana-video-2b-longlive

ğŸ” Verifying 1 model(s) for testing...
   âœ… sana-video-2b-longlive: SANA-Video
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - sana-video-2b-longlive
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: SANA-Video (sana-video-2b-longlive)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with sana-video-2b-longlive
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 177, in run
    wrapper_class = _load_model_wrapper(model_name)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 42, in _load_model_wrapper
    module = importlib.import_module(config["wrapper_module"])
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/sana_inference.py", line 83, in <module>
    from diffusers import SanaImageToVideoPipeline
ImportError: cannot import name 'SanaImageToVideoPipeline' from 'diffusers' (/home/ubuntu/Hokin/VMEvalKit/envs/sana-video-2b-longlive/lib/python3.10/site-packages/diffusers/__init__.py)
   âŒ sana-video-2b-longlive: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: wan-2.2-i2v-a14b
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating wan-2.2-i2v-a14b... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): wan-2.2-i2v-a14b

ğŸ” Verifying 1 model(s) for testing...
   âœ… wan-2.2-i2v-a14b: WAN (Wan-AI)
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - wan-2.2-i2v-a14b
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: WAN (Wan-AI) (wan-2.2-i2v-a14b)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with wan-2.2-i2v-a14b
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: wan-2.2-i2v-a14b (will be reused for all tasks)
Fetching 41 files:   0%|          | 0/41 [00:00<?, ?it/s]Fetching 41 files:   5%|â–         | 2/41 [00:00<00:09,  3.96it/s]Fetching 41 files:   7%|â–‹         | 3/41 [00:00<00:08,  4.31it/s]Fetching 41 files:  10%|â–‰         | 4/41 [00:06<01:30,  2.45s/it]Fetching 41 files:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [00:07<00:13,  2.12it/s]Fetching 41 files:  34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [00:08<00:13,  1.95it/s]Fetching 41 files:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [00:09<00:09,  2.33it/s]Fetching 41 files:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [00:14<00:21,  1.01it/s]Fetching 41 files:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [00:14<00:18,  1.13it/s]Fetching 41 files:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [00:15<00:15,  1.28it/s]Fetching 41 files:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22/41 [00:15<00:12,  1.53it/s]Fetching 41 files:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [00:16<00:13,  1.30it/s]Fetching 41 files:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [00:17<00:05,  2.48it/s]Fetching 41 files:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [00:21<00:12,  1.06s/it]Fetching 41 files:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 30/41 [00:22<00:10,  1.05it/s]Fetching 41 files:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [00:23<00:08,  1.13it/s]Fetching 41 files:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [00:23<00:04,  1.60it/s]Fetching 41 files:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 34/41 [00:24<00:05,  1.36it/s]Fetching 41 files:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [00:24<00:02,  1.87it/s]Fetching 41 files:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [00:27<00:03,  1.07it/s]Fetching 41 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:27<00:00,  1.50it/s]
Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s][A
Loading checkpoint shards:   8%|â–Š         | 1/12 [00:00<00:01,  6.16it/s][A
Loading checkpoint shards:  17%|â–ˆâ–‹        | 2/12 [00:00<00:01,  6.54it/s][A
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:00<00:01,  6.61it/s][A
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 4/12 [00:00<00:01,  6.56it/s][A
Loading checkpoint shards:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:00<00:01,  6.54it/s][A
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:00<00:00,  6.44it/s][A
Loading checkpoint shards:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:01<00:00,  6.25it/s][A
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:01<00:00,  6.21it/s][A
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:01<00:00,  6.14it/s][A
Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 10/12 [00:01<00:00,  6.02it/s][A
Loading checkpoint shards:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:01<00:00,  6.05it/s][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00,  6.44it/s]
Loading pipeline components...:  17%|â–ˆâ–‹        | 1/6 [00:02<00:10,  2.17s/it]Loading pipeline components...:  33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [00:02<00:05,  1.26s/it]`torch_dtype` is deprecated! Use `dtype` instead!

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 100.13it/s]

Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s][A
Loading checkpoint shards:   8%|â–Š         | 1/12 [00:00<00:01,  7.00it/s][A
Loading checkpoint shards:  17%|â–ˆâ–‹        | 2/12 [00:00<00:01,  6.90it/s][A
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:00<00:01,  6.50it/s][A
Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 4/12 [00:00<00:01,  6.42it/s][A
Loading checkpoint shards:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:00<00:01,  6.51it/s][A
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:00<00:00,  6.52it/s][A
Loading checkpoint shards:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:01<00:00,  6.52it/s][A
Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:01<00:00,  6.50it/s][A
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:01<00:00,  6.45it/s][A
Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 10/12 [00:01<00:00,  6.41it/s][A
Loading checkpoint shards:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:01<00:00,  6.22it/s][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00,  6.83it/s][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:01<00:00,  6.58it/s]
Loading pipeline components...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:04<00:02,  1.10s/it]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.63it/s]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.22it/s]
  0%|          | 0/50 [00:00<?, ?it/s]  2%|â–         | 1/50 [00:37<30:40, 37.56s/it]  4%|â–         | 2/50 [01:15<30:01, 37.53s/it]  6%|â–Œ         | 3/50 [01:52<29:29, 37.65s/it]  8%|â–Š         | 4/50 [02:30<28:48, 37.57s/it] 10%|â–ˆ         | 5/50 [03:07<28:08, 37.53s/it] 12%|â–ˆâ–        | 6/50 [03:45<27:29, 37.50s/it] 14%|â–ˆâ–        | 7/50 [04:22<26:51, 37.48s/it] 16%|â–ˆâ–Œ        | 8/50 [05:00<26:12, 37.45s/it] 18%|â–ˆâ–Š        | 9/50 [05:37<25:34, 37.43s/it] 20%|â–ˆâ–ˆ        | 10/50 [06:14<24:56, 37.41s/it] 22%|â–ˆâ–ˆâ–       | 11/50 [06:52<24:18, 37.40s/it] 24%|â–ˆâ–ˆâ–       | 12/50 [07:29<23:40, 37.39s/it] 26%|â–ˆâ–ˆâ–Œ       | 13/50 [08:06<23:03, 37.39s/it] 28%|â–ˆâ–ˆâ–Š       | 14/50 [08:44<22:26, 37.40s/it] 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [09:21<21:49, 37.41s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [09:59<21:11, 37.40s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [10:36<20:34, 37.41s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [11:14<19:57, 37.42s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [11:51<19:19, 37.41s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [12:28<18:41, 37.39s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [13:06<18:03, 37.37s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [13:43<17:25, 37.34s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [14:20<16:47, 37.32s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [14:57<16:10, 37.32s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [15:35<15:33, 37.32s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [16:12<14:55, 37.33s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [16:49<14:18, 37.34s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [17:27<13:41, 37.35s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [18:04<13:04, 37.34s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [18:41<12:26, 37.34s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [19:19<11:49, 37.34s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [19:56<11:12, 37.33s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [20:33<10:34, 37.33s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [21:11<09:57, 37.33s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [21:48<09:19, 37.33s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [22:25<08:42, 37.33s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [23:03<08:05, 37.32s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [23:40<07:27, 37.31s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [24:17<06:50, 37.31s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [24:55<06:13, 37.31s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [25:32<05:35, 37.29s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [26:09<04:58, 37.28s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [26:46<04:20, 37.27s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [27:24<03:43, 37.27s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [28:01<03:06, 37.27s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [28:38<02:29, 37.27s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [29:15<01:51, 37.26s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [29:53<01:14, 37.27s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [30:30<00:37, 37.27s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [31:07<00:00, 37.26s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [31:07<00:00, 37.36s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/wan-2.2-i2v-a14b/tests_task/tests_0001/wan-2.2-i2v-a14b_tests_0001_20251218_003853
   - Video: data/outputs/pilot_experiment/wan-2.2-i2v-a14b/tests_task/tests_0001/wan-2.2-i2v-a14b_tests_0001_20251218_003853/video/
   - Question data: data/outputs/pilot_experiment/wan-2.2-i2v-a14b/tests_task/tests_0001/wan-2.2-i2v-a14b_tests_0001_20251218_003853/question/
   - Metadata: data/outputs/pilot_experiment/wan-2.2-i2v-a14b/tests_task/tests_0001/wan-2.2-i2v-a14b_tests_0001_20251218_003853/metadata.json
     âœ… Success! Structured output saved to: data/outputs/pilot_experiment/wan-2.2-i2v-a14b/tests_task/tests_0001/wan-2.2-i2v-a14b_tests_0001_20251218_003853
      âœ… Completed successfully
    [2/2] Processing: tests_0002

  ğŸ¬ Generating: tests_0002 with wan-2.2-i2v-a14b
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0002/first_frame.png
     Prompt: The image shows a horizontally flipped clock. After 2 hours passes on the origin...
  0%|          | 0/50 [00:00<?, ?it/s]  2%|â–         | 1/50 [00:37<30:30, 37.36s/it]  4%|â–         | 2/50 [01:14<29:53, 37.37s/it]  6%|â–Œ         | 3/50 [01:52<29:15, 37.36s/it]  8%|â–Š         | 4/50 [02:29<28:38, 37.36s/it] 10%|â–ˆ         | 5/50 [03:06<28:01, 37.36s/it] 12%|â–ˆâ–        | 6/50 [03:44<27:23, 37.36s/it] 14%|â–ˆâ–        | 7/50 [04:21<26:46, 37.35s/it] 16%|â–ˆâ–Œ        | 8/50 [04:58<26:08, 37.35s/it] 18%|â–ˆâ–Š        | 9/50 [05:36<25:30, 37.34s/it] 20%|â–ˆâ–ˆ        | 10/50 [06:13<24:53, 37.33s/it] 22%|â–ˆâ–ˆâ–       | 11/50 [06:50<24:15, 37.32s/it] 24%|â–ˆâ–ˆâ–       | 12/50 [07:28<23:38, 37.33s/it] 26%|â–ˆâ–ˆâ–Œ       | 13/50 [08:05<23:01, 37.33s/it] 28%|â–ˆâ–ˆâ–Š       | 14/50 [08:42<22:23, 37.32s/it] 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [09:20<21:45, 37.31s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [09:57<21:08, 37.31s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [10:34<20:31, 37.30s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [11:11<19:53, 37.29s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [11:49<19:16, 37.30s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [12:26<18:39, 37.30s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [13:03<18:01, 37.31s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [13:41<17:24, 37.32s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [14:18<16:47, 37.32s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [14:55<16:10, 37.34s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [15:33<15:33, 37.36s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [16:10<14:56, 37.37s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [16:48<14:19, 37.37s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [17:25<13:41, 37.36s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [18:02<13:04, 37.35s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [18:40<12:26, 37.35s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [19:17<11:49, 37.34s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [19:54<11:11, 37.33s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [20:32<10:34, 37.33s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [21:09<09:57, 37.32s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [21:46<09:19, 37.33s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [22:24<08:42, 37.33s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [23:01<08:05, 37.33s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [23:38<07:27, 37.33s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [24:16<06:50, 37.33s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [24:53<06:13, 37.34s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [25:30<05:35, 37.33s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [26:07<04:58, 37.32s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [26:45<04:21, 37.29s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [27:22<03:43, 37.29s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [27:59<03:06, 37.27s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [28:36<02:29, 37.25s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [29:14<01:51, 37.25s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [29:51<01:14, 37.25s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [30:28<00:37, 37.24s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [31:05<00:00, 37.22s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [31:05<00:00, 37.32s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/wan-2.2-i2v-a14b/tests_task/tests_0002/wan-2.2-i2v-a14b_tests_0002_20251218_011111
   - Video: data/outputs/pilot_experiment/wan-2.2-i2v-a14b/tests_task/tests_0002/wan-2.2-i2v-a14b_tests_0002_20251218_011111/video/
   - Question data: data/outputs/pilot_experiment/wan-2.2-i2v-a14b/tests_task/tests_0002/wan-2.2-i2v-a14b_tests_0002_20251218_011111/question/
   - Metadata: data/outputs/pilot_experiment/wan-2.2-i2v-a14b/tests_task/tests_0002/wan-2.2-i2v-a14b_tests_0002_20251218_011111/metadata.json
     âœ… Success! Structured output saved to: data/outputs/pilot_experiment/wan-2.2-i2v-a14b/tests_task/tests_0002/wan-2.2-i2v-a14b_tests_0002_20251218_011111
      âœ… Completed successfully

  ğŸ“Š Model WAN (Wan-AI) Summary: 2 completed, 0 failed, 0 skipped in 1h 3m 40s

â±ï¸  Sequential execution completed in 1h 3m 40s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 2 (100.0%)
   Failed: 0 (0.0%)
   Skipped: 0 (0.0%)
   â±ï¸ Duration: 1h 3m 40s

ğŸ¯ Results by Domain:
   Tests: âœ… 2 completed | âŒ 0 failed | â­ï¸  0 skipped

ğŸ¤– Results by Model:
   wan-2.2-i2v-a14b: âœ… 2 | âŒ 0 | â­ï¸  0

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âœ… wan-2.2-i2v-a14b: 2 videos generated âœ“

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: luma-ray-2
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating luma-ray-2... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): luma-ray-2

ğŸ” Verifying 1 model(s) for testing...
   âœ… luma-ray-2: Luma Dream Machine
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - luma-ray-2
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: Luma Dream Machine (luma-ray-2)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with luma-ray-2
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 177, in run
    wrapper_class = _load_model_wrapper(model_name)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 42, in _load_model_wrapper
    module = importlib.import_module(config["wrapper_module"])
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/luma_inference.py", line 19, in <module>
    from ..utils.s3_uploader import S3ImageUploader
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/utils/__init__.py", line 3, in <module>
    from .video_decomposer import decompose_video
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/utils/video_decomposer.py", line 20, in <module>
    import matplotlib.pyplot as plt
ModuleNotFoundError: No module named 'matplotlib'
   âŒ luma-ray-2: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: luma-ray-flash-2
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating luma-ray-flash-2... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): luma-ray-flash-2

ğŸ” Verifying 1 model(s) for testing...
   âœ… luma-ray-flash-2: Luma Dream Machine
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - luma-ray-flash-2
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: Luma Dream Machine (luma-ray-flash-2)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with luma-ray-flash-2
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 177, in run
    wrapper_class = _load_model_wrapper(model_name)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 42, in _load_model_wrapper
    module = importlib.import_module(config["wrapper_module"])
  File "/usr/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/luma_inference.py", line 19, in <module>
    from ..utils.s3_uploader import S3ImageUploader
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/utils/__init__.py", line 3, in <module>
    from .video_decomposer import decompose_video
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/utils/video_decomposer.py", line 20, in <module>
    import matplotlib.pyplot as plt
ModuleNotFoundError: No module named 'matplotlib'
   âŒ luma-ray-flash-2: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: veo-2
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating veo-2... (timeout: 10800s)

2025-12-18 01:42:40,357 | INFO     | VeoService initialized with model: veo-2.0-generate-001
2025-12-18 01:42:40,362 | INFO     | Padded image from 276Ã—276 to 490Ã—276 for 16:9 ratio
2025-12-18 01:42:40,366 | INFO     | Starting Veo video generation with model: veo-2.0-generate-001
2025-12-18 01:42:40,366 | INFO     | Prompt: Remove all cone objects from the scene. Do not do anything to other objects.
2025-12-18 01:42:40,788 | INFO     | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/veo-2.0-generate-001:predictLongRunning "HTTP/1.1 200 OK"
2025-12-18 01:42:40,789 | INFO     | Started operation: models/veo-2.0-generate-001/operations/sm1zgrgj4x6g
2025-12-18 01:42:40,789 | INFO     | Still running... attempt 1, elapsed: 0.0s
ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): veo-2

ğŸ” Verifying 1 model(s) for testing...
   âœ… veo-2: Google Veo
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - veo-2
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: Google Veo (veo-2)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with veo-2
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: veo-2 (will be reused for all tasks)
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 202, in run
    result = wrapper.generate(image_path, text_prompt, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/veo_inference.py", line 442, in generate
    video_bytes, metadata = asyncio.run(
  File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/usr/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/veo_inference.py", line 330, in generate_video
    operation = self.client.operations.get(operation_name)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/veo-2/lib/python3.10/site-packages/google/genai/operations.py", line 454, in get
    operation_name = operation.name
AttributeError: 'str' object has no attribute 'name'
   âŒ veo-2: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: veo-3.0-generate
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating veo-3.0-generate... (timeout: 10800s)

2025-12-18 01:42:47,305 | INFO     | VeoService initialized with model: veo-3.0-generate-001
2025-12-18 01:42:47,311 | INFO     | Padded image from 276Ã—276 to 490Ã—276 for 16:9 ratio
2025-12-18 01:42:47,314 | INFO     | Starting Veo video generation with model: veo-3.0-generate-001
2025-12-18 01:42:47,315 | INFO     | Prompt: Remove all cone objects from the scene. Do not do anything to other objects.
2025-12-18 01:42:47,905 | INFO     | HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/veo-3.0-generate-001:predictLongRunning "HTTP/1.1 200 OK"
2025-12-18 01:42:47,905 | INFO     | Started operation: models/veo-3.0-generate-001/operations/wz515h7k47ls
2025-12-18 01:42:47,905 | INFO     | Still running... attempt 1, elapsed: 0.0s
ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): veo-3.0-generate

ğŸ” Verifying 1 model(s) for testing...
   âœ… veo-3.0-generate: Google Veo
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - veo-3.0-generate
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: Google Veo (veo-3.0-generate)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with veo-3.0-generate
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: veo-3.0-generate (will be reused for all tasks)
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 202, in run
    result = wrapper.generate(image_path, text_prompt, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/veo_inference.py", line 442, in generate
    video_bytes, metadata = asyncio.run(
  File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/usr/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/veo_inference.py", line 330, in generate_video
    operation = self.client.operations.get(operation_name)
  File "/home/ubuntu/Hokin/VMEvalKit/envs/veo-3.0-generate/lib/python3.10/site-packages/google/genai/operations.py", line 454, in get
    operation_name = operation.name
AttributeError: 'str' object has no attribute 'name'
   âŒ veo-3.0-generate: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: veo-3.1-fast
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating veo-3.1-fast... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): veo-3.1-fast

ğŸ” Verifying 1 model(s) for testing...
   âœ… veo-3.1-fast: Google Veo 3.1
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - veo-3.1-fast
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: Google Veo 3.1 (veo-3.1-fast)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with veo-3.1-fast
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 188, in run
    self._wrapper_cache[model_name] = wrapper_class(**init_kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/wavespeed_inference.py", line 719, in __init__
    self.veo_service = WaveSpeedService(model=WaveSpeedModel.VEO_3_1_FAST_I2V)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/wavespeed_inference.py", line 70, in __init__
    raise ValueError("WAVESPEED_API_KEY environment variable is required")
ValueError: WAVESPEED_API_KEY environment variable is required
   âŒ veo-3.1-fast: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: wavespeed-wan-2.1-i2v-480p
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating wavespeed-wan-2.1-i2v-480p... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): wavespeed-wan-2.1-i2v-480p

ğŸ” Verifying 1 model(s) for testing...
   âœ… wavespeed-wan-2.1-i2v-480p: WaveSpeed WAN 2.1
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - wavespeed-wan-2.1-i2v-480p
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: WaveSpeed WAN 2.1 (wavespeed-wan-2.1-i2v-480p)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with wavespeed-wan-2.1-i2v-480p
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 188, in run
    self._wrapper_cache[model_name] = wrapper_class(**init_kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/wavespeed_inference.py", line 534, in __init__
    self.wavespeed_service = WaveSpeedService(model=model)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/wavespeed_inference.py", line 70, in __init__
    raise ValueError("WAVESPEED_API_KEY environment variable is required")
ValueError: WAVESPEED_API_KEY environment variable is required
   âŒ wavespeed-wan-2.1-i2v-480p: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: runway-gen4-turbo
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating runway-gen4-turbo... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): runway-gen4-turbo

ğŸ” Verifying 1 model(s) for testing...
   âœ… runway-gen4-turbo: Runway ML
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - runway-gen4-turbo
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: Runway ML (runway-gen4-turbo)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with runway-gen4-turbo
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: runway-gen4-turbo (will be reused for all tasks)
Traceback (most recent call last):
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 683, in <module>
    main()
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 643, in main
    experiment_results = run_pilot_experiment(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 389, in run_pilot_experiment
    result = run_single_inference(
  File "/home/ubuntu/Hokin/VMEvalKit/examples/generate_videos.py", line 247, in run_single_inference
    result = runner.run(
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/runner/inference.py", line 202, in run
    result = wrapper.generate(image_path, text_prompt, **kwargs)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/runway_inference.py", line 417, in generate
    result = asyncio.run(
  File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/usr/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/runway_inference.py", line 227, in generate_video
    image_url = await self._upload_image(processed_image_path)
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/models/runway_inference.py", line 271, in _upload_image
    from ..utils.s3_uploader import S3ImageUploader
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/utils/__init__.py", line 3, in <module>
    from .video_decomposer import decompose_video
  File "/home/ubuntu/Hokin/VMEvalKit/vmevalkit/utils/video_decomposer.py", line 20, in <module>
    import matplotlib.pyplot as plt
ModuleNotFoundError: No module named 'matplotlib'
   âŒ runway-gen4-turbo: FAILED - see output above

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Validating: openai-sora-2
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ Validating openai-sora-2... (timeout: 10800s)

ğŸ” Discovering human-approved tasks from folder structure...
ğŸ” Discovering tasks by scanning actual files: data/questions
   ğŸ“ Scanning tests_task/
      âœ… Found 2 approved tasks in tests

ğŸ“Š Discovery Summary:
   Total approved tasks: 2
   Tests: 2 tasks
   ğŸ¯ Running specific task IDs: tests_0001, tests_0002

ğŸ¯ Selected 1 model(s): openai-sora-2

ğŸ” Verifying 1 model(s) for testing...
   âœ… openai-sora-2: OpenAI Sora
ğŸš€ VMEVAL KIT EXPERIMENT - CLEAN EXECUTION

ğŸ“Š Experiment Configuration:
   Models to run: 1 - openai-sora-2
   Domains: 1
   ğŸ”„ Execution Mode: SEQUENTIAL

ğŸ“ˆ Task Distribution:
   Tests: 2 approved tasks
   Total approved tasks: 2
   Total generations: 2

ğŸ“ Output directory: data/outputs/pilot_experiment
   Skip existing: True

ğŸ“ Output directory structure ready at: data/outputs/pilot_experiment
   Each inference will create a self-contained folder with:
   - video/: Generated video file
   - question/: Input images and prompt
   - metadata.json: Complete inference metadata
ğŸ“ Created per-model folders under: data/outputs/pilot_experiment and mirrored question tasks
ğŸ“‹ Total inference jobs to run: 2
ğŸš€ Starting sequential execution...

   Processing order: Model by model, task by task

ğŸ¤– Processing Model: OpenAI Sora (openai-sora-2)

  ğŸ“š Domain: Tests
    [1/2] Processing: tests_0001

  ğŸ¬ Generating: tests_0001 with openai-sora-2
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0001/first_frame.png
     Prompt: Remove all cone objects from the scene. Do not do anything to other objects....
ğŸ“¦ Loaded model: openai-sora-2 (will be reused for all tasks)

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/openai-sora-2/tests_task/tests_0001/openai-sora-2_tests_0001_20251218_014255
   - Video: data/outputs/pilot_experiment/openai-sora-2/tests_task/tests_0001/openai-sora-2_tests_0001_20251218_014255/video/
   - Question data: data/outputs/pilot_experiment/openai-sora-2/tests_task/tests_0001/openai-sora-2_tests_0001_20251218_014255/question/
   - Metadata: data/outputs/pilot_experiment/openai-sora-2/tests_task/tests_0001/openai-sora-2_tests_0001_20251218_014255/metadata.json
     âœ… Success! Structured output saved to: data/outputs/pilot_experiment/openai-sora-2/tests_task/tests_0001/openai-sora-2_tests_0001_20251218_014255
      âœ… Completed successfully
    [2/2] Processing: tests_0002

  ğŸ¬ Generating: tests_0002 with openai-sora-2
     Image: /lambda/nfs/Hokin/VMEvalKit/data/questions/tests_task/tests_0002/first_frame.png
     Prompt: The image shows a horizontally flipped clock. After 2 hours passes on the origin...

âœ… Inference complete! Output saved to: data/outputs/pilot_experiment/openai-sora-2/tests_task/tests_0002/openai-sora-2_tests_0002_20251218_014442
   - Video: data/outputs/pilot_experiment/openai-sora-2/tests_task/tests_0002/openai-sora-2_tests_0002_20251218_014442/video/
   - Question data: data/outputs/pilot_experiment/openai-sora-2/tests_task/tests_0002/openai-sora-2_tests_0002_20251218_014442/question/
   - Metadata: data/outputs/pilot_experiment/openai-sora-2/tests_task/tests_0002/openai-sora-2_tests_0002_20251218_014442/metadata.json
     âœ… Success! Structured output saved to: data/outputs/pilot_experiment/openai-sora-2/tests_task/tests_0002/openai-sora-2_tests_0002_20251218_014442
      âœ… Completed successfully

  ğŸ“Š Model OpenAI Sora Summary: 2 completed, 0 failed, 0 skipped in 0h 3m 58s

â±ï¸  Sequential execution completed in 0h 3m 58s
ğŸ‰ VIDEO GENERATION COMPLETE!

ğŸ“Š Final Statistics:
   Models tested: 1
   Tasks per model: 2
   Total possible generations: 2
   Total attempted: 2
   Completed: 2 (100.0%)
   Failed: 0 (0.0%)
   Skipped: 0 (0.0%)
   â±ï¸ Duration: 0h 3m 58s

ğŸ¯ Results by Domain:
   Tests: âœ… 2 completed | âŒ 0 failed | â­ï¸  0 skipped

ğŸ¤– Results by Model:
   openai-sora-2: âœ… 2 | âŒ 0 | â­ï¸  0

ğŸ“ All outputs saved to: data/outputs/pilot_experiment
   âœ… openai-sora-2: 2 videos generated âœ“

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Validation Summary
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   âœ… Passed (7):
      âœ“ ltx-video
      âœ“ ltx-video-13b-distilled
      âœ“ svd
      âœ“ cogvideox-5b-i2v
      âœ“ cogvideox1.5-5b-i2v
      âœ“ wan-2.2-i2v-a14b
      âœ“ openai-sora-2

   âŒ Failed (15):
      âœ— morphic-frames-to-video
      âœ— hunyuan-video-i2v
      âœ— dynamicrafter-256
      âœ— dynamicrafter-512
      âœ— dynamicrafter-1024
      âœ— videocrafter2-512
      âœ— sana-video-2b-480p
      âœ— sana-video-2b-longlive
      âœ— luma-ray-2
      âœ— luma-ray-flash-2
      âœ— veo-2
      âœ— veo-3.0-generate
      âœ— veo-3.1-fast
      âœ— wavespeed-wan-2.1-i2v-480p
      âœ— runway-gen4-turbo

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Installation Summary
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   âœ… Installed (22):
      âœ“ ltx-video
      âœ“ ltx-video-13b-distilled
      âœ“ svd
      âœ“ morphic-frames-to-video
      âœ“ hunyuan-video-i2v
      âœ“ dynamicrafter-256
      âœ“ dynamicrafter-512
      âœ“ dynamicrafter-1024
      âœ“ videocrafter2-512
      âœ“ cogvideox-5b-i2v
      âœ“ cogvideox1.5-5b-i2v
      âœ“ sana-video-2b-480p
      âœ“ sana-video-2b-longlive
      âœ“ wan-2.2-i2v-a14b
      âœ“ luma-ray-2
      âœ“ luma-ray-flash-2
      âœ“ veo-2
      âœ“ veo-3.0-generate
      âœ“ veo-3.1-fast
      âœ“ wavespeed-wan-2.1-i2v-480p
      âœ“ runway-gen4-turbo
      âœ“ openai-sora-2

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… All installations completed successfully
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

